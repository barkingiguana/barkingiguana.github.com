---
title: "Load-balanced highly available MySQL on Ubuntu 8.04"
layout: post
author: Craig
---
<p>If you followed my previous post about <a href="http://barkingiguana.com/2008/07/07/high-availability-mysql-on-ubuntu-804">high availability MySQL</a> your application now has one less single point of failure. That's good, but what happens when your MySQL cluster begins to get overloaded? By load-balancing MySQL connections between hosts you can more easily accommodate a larger volume of queries.</p>

<div style="text-align: center;"><img src="http://barkingiguana.com/images/6.png" alt="A load balanced database cluster" /></div>

<h4>Requirements</h4>

<p>This article will build on the MySQL cluster introduced in <a href="http://barkingiguana.com/2008/07/07/high-availability-mysql-on-ubuntu-804">my previous post</a>. If you haven't already, set that up. You'll also need another two virtual machines, each with one IP address.</p>

<ul>
<li>193.219.108.239 - lb-db-01 (lb-db-01.vm.xeriom.net)</li>
<li>193.219.108.240 - lb-db-02 (lb-db-02.vm.xeriom.net)</li>
<li>* 193.219.108.241 - db-01 (db-01.vm.xeriom.net)</li>
<li>* 193.219.108.242 - db-02 (db-02.vm.xeriom.net)</li>
<li>* 193.219.108.243 - virtual IP address</li>
</ul>

<p>IP addresses marked with a * are brought over from the previous article.</p>

<p>All boxes have been <a href="http://barkingiguana.com/2008/06/22/firewall-a-pristine-ubuntu-804-box">firewalled</a>. It's just plain common sense.</p>

<h4>We have the technology</h4>

<p>Install Heartbeat and MySQL Proxy on both load balancer boxes.</p>

<pre><code>sudo apt-get install heartbeat mysql-proxy --yes</code></pre>

<h4>Configure and run MySQL Proxy</h4>

<p>Open the firewall on the database boxes to allow the load balancing boxes to connect.</p>

<pre><code># On db-01 and db-02
sudo iptables -I INPUT 4 -p tcp \
  --dport mysql -s lb-db-01.vm.xeriom.net -j ACCEPT
sudo iptables -I INPUT 4 -p tcp \
  --dport mysql -s lb-db-02.vm.xeriom.net -j ACCEPT
sudo sh -c "iptables-save -c > /etc/iptables.rules"</code></pre>

<p>If you followed the previous post you'll probably also want to remove the rule that allowed MySQL access from the test box to the floating IP address on the backend boxes. It's not hugely important at the moment, but it's nice to be neat. When you put this into production it will become much more important to control access to the database boxes.</p>

<pre><code># On db-01 and db-02
sudo iptables -D INPUT -p tcp --dport mysql -s 193.214.108.10 \
  -d 193.214.108.243 -j ACCEPT
sudo sh -c "iptables-save -c > /etc/iptables.rules"</code></pre>

<p>Remember to swap 193.214.108.243 for your floating IP address and 193.214.108.10 for your test box IP address or you'll get a "bad rule" error.</p>

<p>You'll also need to open the MySQL port on the load balancer boxes. Note that MySQL Proxy listens on port 4040, not the regular MySQL port 3306. My test box here is 193.219.108.10 - it should be whichever IP address outside the database cluster that you're going to connect from to test the proxy works.</p>

<pre><code># On lb-db-01
sudo iptables -I INPUT 4 -p tcp \
  --dport 4040 -d lb-db-01.vm.xeriom.net -s 193.219.108.10 -j ACCEPT
sudo sh -c "iptables-save -c > /etc/iptables.rules"</code></pre>

<pre><code># On lb-db-02
sudo iptables -I INPUT 4 -p tcp \
  --dport 4040 -d lb-db-02.vm.xeriom.net -s 193.219.108.10 -j ACCEPT
sudo sh -c "iptables-save -c > /etc/iptables.rules"</code></pre>

<p>Run the proxy on both boxes, telling it the address of the real database servers, then try to connect from the test box.</p>

<pre><code>sudo /usr/sbin/mysql-proxy \
  --proxy-backend-addresses=db-01.vm.xeriom.net:3306 \
  --proxy-backend-addresses=db-02.vm.xeriom.net:3306 \
  --daemon</code></pre>

<pre><code># On the test box
mysql -u some_user -p'some_other_password' -h lb-db-01.vm.xeriom.net
mysql&gt; \q
mysql -u some_user -p'some_other_password' -h lb-db-02.vm.xeriom.net
mysql&gt; \q</code></pre>

<p>You may be told that your load balancer hosts don't have access to the MySQL server. If this happens, login to the MySQL hosts, add a user at the hostname that failed, and try again.</p>

<pre><code>ERROR 1130 (00000): Host 'lb-db-01' is not allowed to connect to this MySQL server</code></pre>

<pre><code># On db-01 and db-02
mysql -u root -p
Enter password: [Enter your MySQL root password]
mysql&gt; grant all on my_application.* to 'some_user'@'lb-db-01' 
  identified by 'some_other_password';
mysql&gt; grant all on my_application.* to 'some_user'@'lb-db-02' 
  identified by 'some_other_password';
mysql&gt; \q</code></pre>

<p>If you got MySQL prompts both times then both proxies are working. Remove the firewall rules that let your test box talk directly to each node and add rules that allow access only to the floating IP address.</p>

<pre><code># On lb-db-01
sudo iptables -D INPUT -p tcp \
  --dport 4040 -d lb-db-01.vm.xeriom.net -s 193.219.108.10 \
  -j ACCEPT
sudo iptables -I INPUT 4 -p tcp \
  --dport 4040 -d 193.219.108.243 -s 193.219.108.10 \
  -j ACCEPT
sudo sh -c "iptables-save -c > /etc/iptables.rules"</code></pre>

<pre><code># On lb-db-02
sudo iptables -D INPUT -p tcp \
  --dport 4040 -d lb-db-02.vm.xeriom.net -s 193.219.108.10 \
  -j ACCEPT
sudo iptables -I INPUT 4 -p tcp \
  --dport 4040 -d 193.219.108.243 -s 193.219.108.10 \
  -j ACCEPT
sudo sh -c "iptables-save -c > /etc/iptables.rules"</code></pre>

<h4>Configure and run Heartbeat</h4>

<p>Now it's time to configure Heartbeat on both boxes. Open up the firewall and then populate Heartbeat's configuration files.</p>

<pre><code># On lb-db-01
sudo iptables -I INPUT 4 -p udp \
  --dport 694 -s lb-db-02.vm.xeriom.net -j ACCEPT
sudo sh -c "iptables-save -c > /etc/iptables.rules"</code></pre>

<pre><code># On lb-db-02
sudo iptables -I INPUT 4 -p udp \
  --dport 694 -s lb-db-01.vm.xeriom.net -j ACCEPT
sudo sh -c "iptables-save -c > /etc/iptables.rules"</code></pre>

<pre><code># On both load balancer boxes
sudo cp /usr/share/doc/heartbeat/authkeys /etc/ha.d/
sudo sh -c "zcat /usr/share/doc/heartbeat/ha.cf.gz > /etc/ha.d/ha.cf"
sudo sh -c "zcat /usr/share/doc/heartbeat/haresources.gz > /etc/ha.d/haresources"</code></pre>

<p>The <code>authkeys</code> should be readable only by root because it's going to contain a valuable password.</p>

<pre><code>sudo chmod go-wrx /etc/ha.d/authkeys</code></pre>

<p>Edit <code>/ec/ha.d/authkeys</code> and add a password of your choice so that it looks like below.</p>

<pre><code>auth 2
2 sha1 your-password-here</code></pre>

<p>Configure <code>ha.cf</code> according to your network. In this case the nodes are <code>lb-db-01.vm.xeriom.net</code> and <code>lb-db-02.vm.xeriom.net</code>. To figure out what your node names are run <code>uname -n</code> on each of the nodes. These <strong>must</strong> match the values you use in the <code>node</code> directives in the configuration file.</p>

<pre><code>logfile /var/log/ha-log
logfacility local0
keepalive 2
deadtime 30
initdead 120
bcast eth0
udpport 694
auto_failback on
node lb-db-01.vm.xeriom.net
node lb-db-02.vm.xeriom.net</code></pre>

<p>Tell Heartbeat that it will be managing the floating IP address with lb-db-01 being the preferred node by editing <code>/etc/ha.d/haresources</code>. Remember that this file must be <em>identical</em> on both boxes.</p>

<pre><code>lb-db-01.vm.xeriom.net 193.219.108.243</code></pre>

<p>If you've had Heartbeat running on the database boxes (as will be the case from the last article) then nuke it now.</p>

<pre><code># On the database boxes
sudo apt-get uninstall heartbeat</code></pre>

<p>Then remove the alias from eth0 on both boxes.</p>

<pre><code># On the database boxes
sudo ifconfig eth0 inet 193.219.108.243 -alias</code></pre>

<p>Now we're ready to fire up Heartbeat on the load balancer boxes.</p>

<pre><code># On lb-db-01 then lb-db-02
sudo /etc/init.d/heartbeat restart</code></pre>

<h4>Testing, testing, testing</h4>

<p>Fire up mysql on the test box and connect to the floating IP address. You should get the MySQL command prompt.</p>

<pre><code>mysql -u some_user -p'some_other_password' -h 193.214.108.243 my_application</code></pre>

<p>Typing out exactly what is done to test this would take a long time and, largely, would be a waste of space. Here's a summary of the procedure. At all stages you should get a result from your query.</p>

<ol>
<li>Run a query such as <code>show processlist;</code></li>
<li>Shutdown db-01</li>
<li>Run the query again</li>
<li>Start db-01</li>
<li>Shutdown db-02</li>
<li>Run the query again</li>
<li>Start db-02</li>
<li>Shutdown lb-db-01</li>
<li>Run the query again</li>
<li>Shutdown db-01</li>
<li>Run the query again</li>
<li>Start db-01</li>
<li>Shutdown db-02</li>
<li>Run the query again</li>
<li>Start db-02</li>
<li>Start lb-db-01</li>
<li>Run the query again</li>
</ol>

<p>If your query ran successfully each time then congratulations, you've now got a load balanced, highly available, MySQL instance.</p>

<h4>Where now?</h4>

<p>Being highly available and load balanced doesn't protect you from mistakes. Backup often, and check you can restore from your backups. You may be interested in building a MySQL binlog-only server to get point-in-time recovery.</p>

<p>MySQL Proxy talks <a href="http://www.lua.org/">Lua</a>. Consider learning how to write it.</p>

<p>I've not yet documented how to take the cluster beyond two load balancers and two database nodes. It's possible, but it shouldn't be used as a solution to scaling the setup I've described without some research. Instead of expanding beyond two nodes in a master-master cluster it may be more suitable to setup several master-master nodes and shard or federate your data. It may be that you need to rearrange your schema or play with master-slave replication and do some tricks on the slave to make reads faster. How you scale your database depends on your data and how you use it. Do your homework... and be sure to blog about it and let me know how it goes.</p>

<h4>Thinking of a title is the hardest part</h4>

<p>If you found this article useful, give me some love over at <a href="http://www.workingwithrails.com/recommendation/new/person/7241-craig-webster">Working With Rails</a>. If I get 100 points then I get to live.</p>
